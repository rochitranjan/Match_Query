{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a1f19f",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a718c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rochitranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rochitranjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import itertools \n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from datetime import timedelta\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a98898",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b49ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kristal = pd.read_excel('C:/Users/rochitranjan/Documents/kristal/Kristal.xlsx', sheet_name = 'Sheet1')\n",
    "\n",
    "def find_pattern(patterns, text):\n",
    "    if re.findall(patterns, text):\n",
    "        return re.findall(patterns, text)\n",
    "    else:\n",
    "        return 'Not Found!'\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "kristal['contains_dollar'] = kristal['product_price'].apply(lambda x : 1 if '$' in x else 0 )\n",
    "\n",
    "\n",
    "#ASSUMPTION : for the products have multiple price listed in the 'product_price' column the first one is the right price. \n",
    "#This functionality can further be enhanced using IOB labelling to indentify the Selling price and the List Price\n",
    "kristal['price'] =kristal['product_price'].apply(lambda x : find_pattern( \"\\$([0-9,]*\\.[0-9]*)\", x)[0] )\n",
    "\n",
    "kristal.drop(labels =['contains_dollar','product_price'], axis = 1, inplace = True)\n",
    "\n",
    "kristal['product_title']=kristal['product_title'].apply(lambda x : str(x).lower())\n",
    "\n",
    "#kristal['product_title'] = kristal['product_title'].apply(lambda x : re.sub(r'[^\\w\\s]', '', str(x)))\n",
    "\n",
    "kristal['tokens'] = kristal['product_title'].apply(lambda x : word_tokenize(str(x)))\n",
    "\n",
    "kristal['tokens'] = kristal['tokens'].apply(lambda x : list(filter(lambda token: token not in string.punctuation, x)))\n",
    "\n",
    "kristal['tokens'] = kristal['tokens'].apply(lambda x : list(filter(lambda token: token not in stopwords, x)))\n",
    "\n",
    "kristal['tokens'] = kristal['tokens'].apply(lambda tokens : word_tokenize(re.sub(r'[^\\w\\s]', '', str(' '.join(tokens)))))\n",
    "\n",
    "kristal['tokens'] = kristal['tokens'].apply(lambda tokens : word_tokenize(re.sub(r'[0-9]+', '', str(' '.join(tokens)))))\n",
    "\n",
    "kristal['tokens'] = kristal['tokens'].apply(lambda x : list(filter(lambda token: len(token) >1 , x)))\n",
    "\n",
    "vocab = sum(kristal['tokens'].tolist(), [])\n",
    "\n",
    "with open('C:/Users/rochitranjan/Documents/kristal/vocab.txt', 'w', encoding=\"utf-8\") as fp:\n",
    "    fp.write(str(vocab))\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38695557",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dct = dict(Counter(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d688303",
   "metadata": {},
   "source": [
    "### Find Products in the Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6a7b38",
   "metadata": {},
   "source": [
    "`Run the below function to find products in the Dataframe if the search token is present in the vocab.txt file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1928134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_products(search_item, df_inp):\n",
    "    search_item = search_item.lower()\n",
    "    df = df_inp.copy()\n",
    "    df['count'] = 0\n",
    "    if search_item in vocab:\n",
    "        df['count'] = df['tokens'].apply(lambda tokens : tokens.count(search_item))\n",
    "    df = df[df['count'] >= 1]\n",
    "    df.sort_values(by = ['count', 'price'], ascending = [False, True], inplace = True)\n",
    "    if len(df) >= 1:\n",
    "        return df.iloc[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd52050",
   "metadata": {},
   "source": [
    "`Run the below function to clean search token if the search token is NOT present in the vocab.txt file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa997e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_search_item(search_item):\n",
    "    punct = list(string.punctuation)\n",
    "    punct.extend(list(range(10)))\n",
    "    search_item = ''.join(list(filter(lambda char: char not in punct, search_item)))\n",
    "    return search_item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac6c90",
   "metadata": {},
   "source": [
    "`Run the below functions to run fuzzy matching logic if search token is NOT present in the vocab.txt file. This logic suggests  the correct word basis one or two edits(of the original search token) and the probability of occurence of the suggested edits in the vocab.txt file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "08c47699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits_one(word):\n",
    "    \"Create all edits that are one edit away from `word`.\"\n",
    "    alphabets    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])                   for i in range(len(word) + 1)]\n",
    "    deletes    = [left + right[1:]                       for left, right in splits if right]\n",
    "    inserts    = [left + c + right                       for left, right in splits for c in alphabets]\n",
    "    replaces   = [left + c + right[1:]                   for left, right in splits if right for c in alphabets]\n",
    "    transposes = [left + right[1] + right[0] + right[2:] for left, right in splits if len(right)>1]\n",
    "    return set(deletes + inserts + replaces + transposes)\n",
    "\n",
    "def edits_two(word):\n",
    "    \"Create all edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits_one(word) for e2 in edits_one(e1))\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the `all_words`.\"\n",
    "    return set(word for word in words if word in all_words)\n",
    "\n",
    "def possible_corrections(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits_one(word)) or known(edits_two(word)) or [word])\n",
    "\n",
    "def prob(word, N=sum(all_words.values())): \n",
    "    \"Probability of `word`: Number of appearances of 'word' / total number of tokens\"\n",
    "    return all_words[word] / N\n",
    "\n",
    "\n",
    "def spell_check(word):\n",
    "    \"Print the most probable spelling correction for `word` out of all the `possible_corrections`\"\n",
    "    correct_word = max(possible_corrections(word), key=prob)\n",
    "    if correct_word != None:\n",
    "        return correct_word\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1a85d",
   "metadata": {},
   "source": [
    "### Test Case 1: When search token does not have punctuation and it is NOT in the VOCAB.TXT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53a16896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paystation', 'plantation', 'playstation', 'station'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_item = 'plstation'\n",
    "search_item = search_item.lower()\n",
    "all_words = Counter(vocab)\n",
    "possible_corrections(search_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd533b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the unofficial playstation handbook : a guide to using playstation 4, playstation tv, and playstation 3 (paperback)\n",
      "14.99\n"
     ]
    }
   ],
   "source": [
    "# test spel\n",
    "correct_word = spell_check('plstation')\n",
    "if correct_word != None:\n",
    "    result = find_products(correct_word, kristal)\n",
    "    print(result['product_title'])\n",
    "    print(result['price'])\n",
    "else:\n",
    "    print('No match Found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7201f18",
   "metadata": {},
   "source": [
    "### Test Case 2: When search token does have a punctuation and it is NOT in the VOCAB.TXT file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8cbe094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24k gold ballpoint eye cream antidark circle aging wrinkle collagen essence care\n",
      "3.61\n"
     ]
    }
   ],
   "source": [
    "correct_word = spell_check('anti-dark')\n",
    "result = find_products(correct_word, kristal)\n",
    "print(result['product_title'])\n",
    "print(result['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432663a8",
   "metadata": {},
   "source": [
    "FUTURE WORK: \n",
    "1. The search token can be enhanced to support multiple word search and incoporate the word ordering to the tokens present in the search token.\n",
    "2. Similarity Ratio(E.g Levenshtein Similarity Ratio/Jaccards Ratio etc) can also be utilised to enhance the spell correction feature of the proposed solution.\n",
    "3. NER techniques can be utilised to enhance the price extraction feature of the proposed solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
